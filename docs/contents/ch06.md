# 第6章 工业界前沿技术应用

## 1 强化学习

1. 特点：
   - 试错学习
   - 延迟反馈
   - 时间是一个重要因素
   - 当前的行为影响后续接收到的数据

2. 基本组成要素
    - agent（智能体）
    - environment（环境）
    - state（状态）
    - action（动作）
    - reward（奖励）

3. 训练过程：整个训练过程符合马尔可夫决策过程，核心思想是下一步的`state`只和当前的`state`以及要采取的`action`有关，只回溯一步。

### 1.1 基于价值的强化学习

#### 1.1.1 基于价值的算法

- 说明：基于每个`state`下可以采取的所有`action`，以及这些`action`对应的`value`，来选择当前`state`如何行动。
- 代表性算法：Q-Learning、SARSA
- 探索与利用：在强化学习训练的时候，一开始会让`agent`偏向于探索（explore），等到训练多轮之后，尝试了在各种`state`下选择各种`action`，大幅降低探索比例，尽量让`agent`偏向于利用（exploit），选择`value`最大的`action`。

#### 1.1.2 Q-Learning算法

**基本概念：**
- Q-Table：`action`和`value`的键值字典。
- Bellman方程的思想：当在特定时间点和状态下去考虑下一步的决策，不仅要关注当前决策立即产生的reward，也要考虑当前决策衍生的持续性reward。
- Bellman方程：
$$
Q(s,a) \leftarrow Q(s, a) + \alpha [\text{reward} + \gamma \text{Max}_{\alpha'} Q(s', a') - Q(s, a)]
$$
其中
  - $Q(s,a)$表示状态$s$，采取了动作$a$，得到的`value`值。
  - $Q(s', a')$表示状态$s'$，采取了动作$a'$，得到的`value`值。
  - `reward`表示在状态$s$变为状态$s$，环境立即给予的`reward`反馈。
  - $\text{Max}_{\alpha'} Q(s', a')$表示在状态$s'$下，可以采取的所有动作中$a'$，能够得到的最大`value`值。
  - $\alpha$是学习率，$\gamma$是折扣率。

### 1.2 强化学习实际开展的难点

- `reward`设置：无法将环境的反馈量化。
- 采样训练耗时过长，工业实际应用难。
- 容易导致局部最优。

## 2 联邦学习

### 2.1 联邦学习概述

- 定义（来源于《联邦学习白皮书》）：在进行机器学习过程中，各参与方可借助其他方数据进行联合建模。各方无须共享数据资源，即在数据不出本地的情况下，进行联合训练，建立共享的机器学习模型。
- 分类：
  - 横向联邦学习：适用于两个场景下用户的特征重合度高，但用户重合度很低的情况。
  - 纵向联邦学习：适用于两个场景下用户的特征重合度低，但用户重合度很高的情况。
  - 联邦迁移学习：适用于两个场景下用户的特征和用户本身重合度都很低的情况。
- 特征：多方协作、各方平等、数据隐私保护、数据加密。

### 2.2 横向联邦学习

- 
